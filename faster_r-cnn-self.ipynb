{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载必备库文件\n",
    "import numpy as np\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import init\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res18_blk_num  = [2, 2, 2,  2] # 不是 bottleneck\n",
    "res50_blk_num  = [3, 4, 6,  3]\n",
    "res101_blk_num = [3, 4, 23, 3]\n",
    "res152_blk_num = [3, 8, 36, 3]\n",
    "\n",
    "# 为了便于参数导入，变量名最好和 gluoncv 的变量名相同\n",
    "class BottleneckV1b(gluon.HybridBlock):\n",
    "    \"\"\"res 基本模块\"\"\"\n",
    "    def __init__(self, channels, strides, isdownsample=False, **kwargs):\n",
    "        super(BottleneckV1b, self).__init__(**kwargs)\n",
    "        # 一个 bottleneck 内， 1*1 卷积 channel 扩大的倍数\n",
    "        self.expansion = 4\n",
    "        self.isdownsample = isdownsample\n",
    "        self.strides = strides\n",
    "\n",
    "        \n",
    "        # bottletneck 内总是 1*1 conv -- 3*3 conv -- 1*1 conv\n",
    "        self.conv1 = nn.Conv2D(channels=channels, kernel_size=(1,1), strides=(1,1),\n",
    "                               padding=(0,0), groups=1, use_bias=False)\n",
    "        self.bn1   = nn.BatchNorm() # use_global_stats=True 默认为 False\n",
    "        self.relu  = nn.Activation('relu')\n",
    "        \n",
    "        self.conv2 = nn.Conv2D(channels=channels, kernel_size=3, strides=self.strides,\n",
    "                               padding=(1,1), groups=1, use_bias=False)\n",
    "        self.bn2   = nn.BatchNorm()\n",
    "        \n",
    "        self.conv3 = nn.Conv2D(channels=channels * self.expansion, kernel_size=(1,1), strides=(1,1),\n",
    "                               padding=(0,0), groups=1, use_bias=False)\n",
    "        self.bn3   = nn.BatchNorm()\n",
    "        \n",
    "        if self.isdownsample:\n",
    "            self.downsample = nn.HybridSequential()\n",
    "            self.downsample.add(nn.Conv2D(channels=channels * self.expansion, kernel_size=(1,1), strides=self.strides,\n",
    "                                         padding=(0,0), groups=1, use_bias=False),\n",
    "                                nn.BatchNorm())\n",
    "\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        residual = self.relu(self.bn1(self.conv1(x)))\n",
    "        residual = self.relu(self.bn2(self.conv2(residual)))\n",
    "        residual = self.bn3(self.conv3(residual))\n",
    "\n",
    "        if self.isdownsample:\n",
    "            x = self.downsample(x)\n",
    "\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BottleneckV1b(64, (2,2), isdownsample=True)\n",
    "x = nd.random.uniform(shape=(1,1,224,224))\n",
    "net.initialize(init.Xavier())\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(gluon.HybridBlock):\n",
    "    \"\"\" Pre-trained ResNetV1b Model, which produces the strides of 8\n",
    "    featuremaps at conv5.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    block : Block\n",
    "        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n",
    "    layers : list of int\n",
    "        Numbers of layers in each block\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    norm_layer : object\n",
    "        Normalization layer used (default: :class:`mxnet.gluon.nn.BatchNorm`)\n",
    "        Can be :class:`mxnet.gluon.nn.BatchNorm` or :class:`mxnet.gluon.contrib.nn.SyncBatchNorm`.\n",
    "    last_gamma : bool, default False\n",
    "        Whether to initialize the gamma of the last BatchNorm layer in each bottleneck to zero.\n",
    "    deep_stem : bool, default False\n",
    "        Whether to replace the 7x7 conv1 with 3 3x3 convolution layers.\n",
    "    avg_down : bool, default False\n",
    "        Whether to use average pooling for projection skip connection between stages/downsample.\n",
    "    final_drop : float, default 0.0\n",
    "        Dropout ratio before the final classification layer.\n",
    "    use_global_stats : bool, default False\n",
    "        Whether forcing BatchNorm to use global statistics instead of minibatch statistics;\n",
    "        optionally set to True if finetuning using ImageNet classification pretrained models.\n",
    "    \"\"\"\n",
    "    # pylint: disable=unused-variable\n",
    "    def __init__(self, block_nums, channels, strides, classes=1000, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = nn.HybridSequential()\n",
    "        self.features.add(nn.Conv2D(channels=64, kernel_size=(7,7), strides=(2,2),\n",
    "                                    padding=(3,3), groups=1, use_bias=False))\n",
    "        self.features.add(nn.BatchNorm())\n",
    "        self.features.add(nn.Activation('relu'))\n",
    "        self.features.add(nn.MaxPool2D(pool_size=(3,3), strides=(2,2), padding=(1,1)))\n",
    "        \n",
    "        # block_nums = [3, 4, 6, 3]\n",
    "        # channels = [64, 128, 256, 512]\n",
    "        # strides = [(1,1), (2,2), (2,2), (2,2)]\n",
    "        for i in range(len(block_nums)):\n",
    "            blk = nn.HybridSequential()\n",
    "            for num in range(block_nums[i]) :\n",
    "                if num == 0:\n",
    "                    bottleneck = BottleneckV1b(channels[i], strides[i], isdownsample=True)\n",
    "                else:\n",
    "                    bottleneck = BottleneckV1b(channels[i], (1,1), isdownsample=False)\n",
    "                blk.add(bottleneck)\n",
    "            self.features.add(blk)\n",
    "\n",
    "        self.avgpool = nn.GlobalAvgPool2D()\n",
    "        self.out = nn.Dense(classes)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        feature = self.features(x)\n",
    "        out = self.avgpool(feature)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "block_nums = [3, 4, 6, 3]\n",
    "channels = [64, 128, 256, 512]\n",
    "strides = [(1,1), (2,2), (2,2), (2,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(block_nums, channels, strides)\n",
    "x = nd.random.uniform(shape=(1,1,224,224))\n",
    "net.initialize(init.Xavier())\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(1 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(64 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (4): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (5): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "    )\n",
      "    (7): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  (out): Dense(2048 -> 1000, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchor_gen(ratios, scales, width, height, stride):\n",
    "    \"\"\"生成 anchor\n",
    "    return (xmin, ymin, xmax, ymax)\"\"\"\n",
    "    anchor = []\n",
    "    for s in scales:\n",
    "        for r in ratios:\n",
    "            w = s / np.sqrt(r)\n",
    "            w = np.round(w * 0.5)\n",
    "            h = s * np.sqrt(r)\n",
    "            h = np.round(h * 0.5)\n",
    "            anchor.append([-w, -h, w, h])\n",
    "            \n",
    "    anchor = np.array(anchor)\n",
    "\n",
    "    x = range(width)\n",
    "    y = range(height)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    offsets = np.concatenate((x[:,:,np.newaxis], y[:,:,np.newaxis], x[:,:,np.newaxis], y[:,:,np.newaxis]), axis = -1)\n",
    "    offsets *= stride\n",
    "    #print(offsets.shape)\n",
    "    #print(offsets)\n",
    "    # 中心点需要向右下角偏移 stride // 2\n",
    "    offsets += stride // 2\n",
    "    #print(offsets)\n",
    "    \n",
    "    anchor = anchor.reshape(1, -1, 4) + offsets.reshape(-1,1,4)\n",
    "    anchor = anchor.reshape(-1, 4)\n",
    "    \n",
    "    return anchor\n",
    "\n",
    "ratios = [0.5, 1, 2]\n",
    "scales = [32, 64, 128, 256, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 37, 4)\n",
      "[[[  0   0   0   0]\n",
      "  [ 16   0  16   0]\n",
      "  [ 32   0  32   0]\n",
      "  ..., \n",
      "  [544   0 544   0]\n",
      "  [560   0 560   0]\n",
      "  [576   0 576   0]]\n",
      "\n",
      " [[  0  16   0  16]\n",
      "  [ 16  16  16  16]\n",
      "  [ 32  16  32  16]\n",
      "  ..., \n",
      "  [544  16 544  16]\n",
      "  [560  16 560  16]\n",
      "  [576  16 576  16]]\n",
      "\n",
      " [[  0  32   0  32]\n",
      "  [ 16  32  16  32]\n",
      "  [ 32  32  32  32]\n",
      "  ..., \n",
      "  [544  32 544  32]\n",
      "  [560  32 560  32]\n",
      "  [576  32 576  32]]\n",
      "\n",
      " ..., \n",
      " [[  0 752   0 752]\n",
      "  [ 16 752  16 752]\n",
      "  [ 32 752  32 752]\n",
      "  ..., \n",
      "  [544 752 544 752]\n",
      "  [560 752 560 752]\n",
      "  [576 752 576 752]]\n",
      "\n",
      " [[  0 768   0 768]\n",
      "  [ 16 768  16 768]\n",
      "  [ 32 768  32 768]\n",
      "  ..., \n",
      "  [544 768 544 768]\n",
      "  [560 768 560 768]\n",
      "  [576 768 576 768]]\n",
      "\n",
      " [[  0 784   0 784]\n",
      "  [ 16 784  16 784]\n",
      "  [ 32 784  32 784]\n",
      "  ..., \n",
      "  [544 784 544 784]\n",
      "  [560 784 560 784]\n",
      "  [576 784 576 784]]]\n",
      "[[[  8   8   8   8]\n",
      "  [ 24   8  24   8]\n",
      "  [ 40   8  40   8]\n",
      "  ..., \n",
      "  [552   8 552   8]\n",
      "  [568   8 568   8]\n",
      "  [584   8 584   8]]\n",
      "\n",
      " [[  8  24   8  24]\n",
      "  [ 24  24  24  24]\n",
      "  [ 40  24  40  24]\n",
      "  ..., \n",
      "  [552  24 552  24]\n",
      "  [568  24 568  24]\n",
      "  [584  24 584  24]]\n",
      "\n",
      " [[  8  40   8  40]\n",
      "  [ 24  40  24  40]\n",
      "  [ 40  40  40  40]\n",
      "  ..., \n",
      "  [552  40 552  40]\n",
      "  [568  40 568  40]\n",
      "  [584  40 584  40]]\n",
      "\n",
      " ..., \n",
      " [[  8 760   8 760]\n",
      "  [ 24 760  24 760]\n",
      "  [ 40 760  40 760]\n",
      "  ..., \n",
      "  [552 760 552 760]\n",
      "  [568 760 568 760]\n",
      "  [584 760 584 760]]\n",
      "\n",
      " [[  8 776   8 776]\n",
      "  [ 24 776  24 776]\n",
      "  [ 40 776  40 776]\n",
      "  ..., \n",
      "  [552 776 552 776]\n",
      "  [568 776 568 776]\n",
      "  [584 776 584 776]]\n",
      "\n",
      " [[  8 792   8 792]\n",
      "  [ 24 792  24 792]\n",
      "  [ 40 792  40 792]\n",
      "  ..., \n",
      "  [552 792 552 792]\n",
      "  [568 792 568 792]\n",
      "  [584 792 584 792]]]\n"
     ]
    }
   ],
   "source": [
    "anchor = anchor_gen(ratios, scales, 600//16, 800//16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200000, 4)\n",
      "[-23. -11.  23.  11.]\n",
      "[-16. -16.  16.  16.]\n",
      "[-11. -23.  11.  23.]\n",
      "[-45. -23.  45.  23.]\n",
      "[-32. -32.  32.  32.]\n"
     ]
    }
   ],
   "source": [
    "print(anchor.shape)\n",
    "print(anchor[0])\n",
    "print(anchor[1])\n",
    "print(anchor[2])\n",
    "print(anchor[3])\n",
    "print(anchor[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_realbbox(bbox_pred, anchors, wh_max=4.42):\n",
    "    \"\"\"根据 anchor 得到预测边框的真实值\n",
    "    bbox_pred, anchors 都是 numpy array 数据类型 (x, y, w, h) 形式，维度 (C,N,4)\"\"\"\n",
    "    batch = bbox_pred.shape[0]\n",
    "\n",
    "    x = bbox_pred[:,:,0] * anchors[:,:,2] + anchors[:,:,0]\n",
    "    y = bbox_pred[:,:,1] * anchors[:,:,3] + anchors[:,:,1]\n",
    "    w = anchors[:,:,2] * np.minimum(np.exp(bbox_pred[:,:,2]), wh_max)\n",
    "    h = anchors[:,:,3] * np.minimum(np.exp(bbox_pred[:,:,3]), wh_max)\n",
    "\n",
    "    return np.concatenate((x.reshape(batch,-1,1),y.reshape(batch,-1,1),\n",
    "                           w.reshape(batch,-1,1),h.reshape(batch,-1,1)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.   0.1  0.2  0.3]\n",
      "  [ 0.4  0.5  0.6  0.7]]\n",
      "\n",
      " [[ 0.8  0.9  1.   1.1]\n",
      "  [ 1.2  1.3  1.4  1.5]]]\n",
      "[[[ 0.   0.3  0.6  0.9]\n",
      "  [ 1.2  1.5  1.8  2.1]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.39      ,  0.73284165,  1.21487293],\n",
       "        [ 1.92      ,  2.55      ,  3.27981384,  4.22888069]],\n",
       "\n",
       "       [[ 0.48      ,  1.11      ,  1.6309691 ,  2.70374942],\n",
       "        [ 3.36      ,  4.23      ,  7.29935994,  9.282     ]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(16).reshape(2,2,4) * 0.1\n",
    "b = np.arange(8).reshape(1,2,4) * 0.3\n",
    "print(a)\n",
    "print(b)\n",
    "get_realbbox(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 为 1 的处理函数\n",
    "def get_realbbox2D(bbox_pred, anchors, wh_max=4.42):\n",
    "    \"\"\"根据 anchor 得到预测边框的真实值\n",
    "    bbox_pred, anchors 都是 numpy array 数据类型 (x, y, w, h) 形式，维度 (N,4)\"\"\"\n",
    "    print(\"bbox_pred\", bbox_pred.shape)\n",
    "    print(\"anchor\", anchors.shape)\n",
    "\n",
    "    x = bbox_pred[:,0] * anchors[:,2] + anchors[:,0]\n",
    "    y = bbox_pred[:,1] * anchors[:,3] + anchors[:,1]\n",
    "    w = anchors[:,2] * np.minimum(np.exp(bbox_pred[:,2]), wh_max)\n",
    "    h = anchors[:,3] * np.minimum(np.exp(bbox_pred[:,3]), wh_max)\n",
    "\n",
    "    return np.concatenate((x.reshape(-1,1),y.reshape(-1,1),\n",
    "                           w.reshape(-1,1),h.reshape(-1,1)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 14.  ,  25.  ,  35.36,  39.78],\n",
       "        [ 11.  ,  20.  ,  22.1 ,  26.52]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box = np.array([[[1,2,3,4],[2,3,4,5]]])\n",
    "anchor = np.array([[[6,7,8,9],[1,2,5,6]]])\n",
    "get_realbbox(box, anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_clip_by_img(bbox, img):\n",
    "    \"\"\"bbox 形式 (x1, y1, x2, y2) 维度 (N,4)\"\"\"\n",
    "    imgsize = img[-2:]\n",
    "    print(imgsize)\n",
    "    bbox[:,:2] = np.maximum(bbox[:,:2], 0)\n",
    "    bbox[:,2] = np.minimum(bbox[:,2], imgsize[0])\n",
    "    bbox[:,3] = np.minimum(bbox[:,3], imgsize[1])\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(bbox_a, bbox_b):\n",
    "    \"\"\"计算两组 bounding boxes 的 Intersection-Over-Union(IOU)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox_a : numpy.ndarray\n",
    "        shape (M, 4) . bbox 格式 (xmin,ymin,xmax,ymax)\n",
    "    bbox_b : numpy.ndarray\n",
    "        shape (N, 4) . bbox 格式 (xmin,ymin,xmax,ymax)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    二维数组 shape (M,N) ，其中任意一个元素 (i,j) 表示 bboxa[i] 和 bboxb[j] 的 IoU\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if bbox_a.shape[1] < 4 or bbox_b.shape[1] < 4:\n",
    "        raise IndexError(\"Bounding boxes axis 1 must have at least length 4\")\n",
    "\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    br = np.minimum(bbox_a[:, None, 2:4], bbox_b[:, 2:4])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:4] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:4] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)\n",
    "\n",
    "\n",
    "# 依据 score 排序，从 score 最高第一个 box 开始，所有与该 box IOU 大于指定阈值的\n",
    "# box 都会被删掉；同时把 box 加入到最终的队列中，并从原 list 中删除。\n",
    "# 接着用队列中剩下的 score 最高的 box 去抑制队列中剩余的 box\n",
    "\n",
    "# 可以先删掉 score 比较低的 box\n",
    "def non_max_suppression(boxes, scores, topk=None, threshold=0.7):\n",
    "    \"\"\"执行 non-maximum suppression ，返回保留 boxes 的索引.\n",
    "    boxes: [N, (y1, x1, y2, x2)]\n",
    "    scores: 1-D array of box scores.\n",
    "    threshold: Float. IoU 阈值，一般为 0.7\n",
    "    \"\"\"\n",
    "    assert boxes.shape[0] > 0\n",
    "    if boxes.dtype.kind != \"f\":\n",
    "        boxes = boxes.astype(np.float32)\n",
    "\n",
    "    # scores 从大到小排序\n",
    "    ixs = scores.argsort()[::-1]\n",
    "    if topk:\n",
    "        ixs = ixs[:topk]\n",
    "    #print(\"ixs\", ixs)\n",
    "\n",
    "    pick = []\n",
    "    while len(ixs) > 0:\n",
    "        # 每次都选择队列中 score 最高的 box ，加入最终结果，并用他抑制队列中剩余的 box\n",
    "        i = ixs[0]\n",
    "        pick.append(i)\n",
    "        # Compute IoU of the picked box with the rest\n",
    "        iou = compute_iou(boxes[i][np.newaxis,:], boxes[ixs[1:]])\n",
    "        # Identify boxes with IoU over the threshold. This\n",
    "        # returns indices into ixs[1:], so add 1 to get\n",
    "        # indices into ixs.\n",
    "        remove_ixs = np.where(iou > threshold)[1] + 1\n",
    "        #print(\"remove_ixs\", remove_ixs)\n",
    "        # Remove indices of the picked and overlapped boxes.\n",
    "        # 所有与 score 最高的 box IoU 大于阈值的 box 都从队列中移除\n",
    "        ixs = np.delete(ixs, remove_ixs)\n",
    "        ixs = np.delete(ixs, 0)\n",
    "    #print(\"pick\",pick)\n",
    "    return np.array(pick, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(anchor, score, bbox_pred, post_nms=300, img=[600,800], nms_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Generate proposals. Limit to batch-size=1 in current implementation.\n",
    "    img = [600,800]\n",
    "    \"\"\"\n",
    "    pre_nms  = 6000\n",
    "    #post_nms = 300\n",
    "    min_size = 60\n",
    "\n",
    "    # 根据 anchor 得到 Bbox 的真实大小\n",
    "    roi = get_realbbox2D(bbox_pred, anchor)\n",
    "\n",
    "    # roi 不能超过图像的边界\n",
    "    roi = bbox_clip_by_img(roi, img)\n",
    "\n",
    "    # remove bounding boxes that don't meet the min_size constraint\n",
    "    width  = roi[:,2] - roi[:,0]\n",
    "    height = roi[:,3] - roi[:,1]\n",
    "    print(\"width\", width)\n",
    "    print(\"height\", height)\n",
    "    invalid = (width < min_size) + (height < min_size)\n",
    "    print(\"invalid\", invalid)\n",
    "    print(\"scores\", score.shape)\n",
    "    invalidindex = np.where(invalid!=0)\n",
    "    score = np.delete(score, invalidindex)\n",
    "    print(\"scores\", score.shape)\n",
    "    print(\"roi\", roi.shape)\n",
    "    roi = np.delete(roi, invalidindex, axis=0)\n",
    "    \n",
    "    print(\"roi\", roi.shape)\n",
    "    print(\"scores\", score.shape)\n",
    "\n",
    "    # Non-maximum suppression\n",
    "    keepindex = non_max_suppression(roi, score, pre_nms, nms_threshold)\n",
    "    \n",
    "    print(\"keepindex\", keepindex.shape)\n",
    "\n",
    "    # 仅仅保留 post_nms 个数的 boxes\n",
    "    if post_nms:\n",
    "        keepindex = keepindex[:post_nms]\n",
    "\n",
    "    print(\"keepindex\", keepindex.shape)\n",
    "\n",
    "    rpn_scores = score[keepindex]\n",
    "    rpn_bbox = roi[keepindex,:]\n",
    "\n",
    "    return rpn_scores, rpn_bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = np.array([[-23., -11.,  23.,  11.],                   \n",
    "                [-16., -16.,  16.,  16.],\n",
    "                [-11., -23.,  11.,  23.],\n",
    "                [-21., -11.,  25.,  11.],\n",
    "                [-14., -16.,  18.,  16.],\n",
    "                [ -9., -23.,  13.,  23.],\n",
    "                [-23.,  -9.,  23.,  13.],\n",
    "                [-16., -14.,  16.,  18.],\n",
    "                [-11., -21.,  11.,  25.],\n",
    "                [-21.,  -9.,  25.,  13.],\n",
    "                [-14., -14.,  18.,  18.],\n",
    "                [ -9., -21.,  13.,  25.]])\n",
    "print(anchor.shape)\n",
    "score = np.array([0.8,0.2,0.4, 0.1,0.45,0.23, 0.45,0.21,0.93, 0.34,0.45,0.42])\n",
    "print(score.shape)\n",
    "bbox_pred = anchor + [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal(anchor, score, bbox_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbox_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRcnn(gluon.HybridBlock):\n",
    "    \"\"\"res50 为 bone 的 faster r-cnn\"\"\"\n",
    "    def __init__(self, class_num=80, **kwargs):\n",
    "        super(FasterRcnn, self).__init__(**kwargs)        \n",
    "        \n",
    "        self.ratios = [0.5, 1, 2]\n",
    "        self.scales = [32, 64, 128, 256, 512]\n",
    "        self.anchor_depth = len(self.ratios) * len(self.scales) # gluoncv 中使用了 15 个 anchor\n",
    "        self.rpn_channels = 1024\n",
    "        self.roi_size = (14, 14)\n",
    "        self.post_nms = 50 #300\n",
    "        self.stride = 16\n",
    "        \n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.socre_thread = 0.01\n",
    "        \n",
    "        # 网络的 bone\n",
    "        bone = ResNet(block_nums, channels, strides)\n",
    "        #self.features = nn.HybridSequential()\n",
    "        #self.features.add(bone.features[:7])\n",
    "        self.features =bone.features[:7]\n",
    "        #self.top_features = nn.HybridSequential()\n",
    "        #self.top_features.add(bone.features[7:])\n",
    "        self.top_features = bone.features[7:]\n",
    "        \n",
    "        self.global_avg_pool = nn.GlobalAvgPool2D()\n",
    "        self.class_predictor = nn.Dense(self.class_num + 1)\n",
    "        self.box_predictor = nn.Dense(self.class_num * 4)\n",
    "        \n",
    "        # RPN 网络\n",
    "        self.rpn = nn.HybridSequential()\n",
    "        self.rpn.add(nn.Conv2D(self.rpn_channels, 3, 1, 1))\n",
    "        self.rpn.add(nn.Activation('relu'))\n",
    "        # 使用了 sigmoid 而不是 softmax，减少通道的个数\n",
    "        self.rpn_score = nn.Conv2D(self.anchor_depth, 1, 1, 0)\n",
    "        self.rpn_loc = nn.Conv2D(self.anchor_depth * 4, 1, 1, 0)\n",
    "\n",
    "        \n",
    "    def hybrid_forward(self, F, x):\n",
    "        feat = self.features(x)\n",
    "        print(\"feat\",feat.shape)\n",
    "        width = 38\n",
    "        height = 50\n",
    "        \n",
    "        # RPN\n",
    "        anchor = anchor_gen(self.ratios, self.scales, width, height, self.stride)\n",
    "        print(\"anchor\", anchor.shape)\n",
    "        rpn = self.rpn(feat)\n",
    "        print(\"rpn\", rpn.shape)\n",
    "        raw_rpn_score = self.rpn_score(rpn).transpose(axes=(0, 2, 3, 1)).reshape((0, -1, 1))\n",
    "        rpn_score = nd.sigmoid(raw_rpn_score)\n",
    "        rpn_box_pred = self.rpn_loc(rpn).transpose(axes=(0, 2, 3, 1)).reshape((0, -1, 4))\n",
    "        print(\"rpn_score\", rpn_score.shape)\n",
    "        print(\"rpn_box_pred\", rpn_box_pred.shape)\n",
    "        rpn_score, rpn_box = proposal(anchor, rpn_score.asnumpy(), np.squeeze(rpn_box_pred.asnumpy()), self.post_nms)\n",
    "        rpn_score = nd.array(rpn_score)\n",
    "        rpn_box = nd.array(rpn_box)\n",
    "        \n",
    "        # roi 开始增加 batchid 0\n",
    "        rpn_box = rpn_box.reshape((-1, 4))\n",
    "        roi_batchid = nd.zeros((self.post_nms, 1))\n",
    "        #roi_batchid = nd.zeros((rpn_box.shape[0], 1))\n",
    "        rpn_roi = nd.concat(*[roi_batchid.reshape((-1, 1)), rpn_box], dim=-1)\n",
    "        \n",
    "        # ROIPolling\n",
    "        pooled_feat = nd.ROIPooling(feat, rpn_roi, self.roi_size, 1. / self.stride)\n",
    "        print(\"pooled_feat\",pooled_feat.shape)\n",
    "\n",
    "        # RCNN prediction\n",
    "        top_feat = self.top_features(pooled_feat)\n",
    "        avg_feat = self.global_avg_pool(top_feat)\n",
    "        cls_pred = self.class_predictor(avg_feat)\n",
    "        box_pred = self.box_predictor(avg_feat)\n",
    "\n",
    "        cls_pred = cls_pred.reshape((self.post_nms, self.class_num + 1))\n",
    "        # 0 是背景的概率，只需要前景的即可\n",
    "        scores = nd.softmax(cls_pred, axis=-1)[1:]\n",
    "        box_pred = box_pred.reshape((self.post_nms, self.class_num, 4)).transpose((1,0,2))\n",
    "        print(\"box_pred\", box_pred.shape)\n",
    "        print(\"rpn_box\", rpn_box.shape)\n",
    "        \n",
    "        # 入参是 center 格式 TODO\n",
    "        bbox = get_realbbox(box_pred.asnumpy(), rpn_box.reshape(1,-1,4).asnumpy(), wh_max=4.42)\n",
    "        print(\"bbox\", bbox.shape)\n",
    "\n",
    "\n",
    "        # 剔除 score 小于 thread 的预测\n",
    "        mask = scores > self.socre_thread\n",
    "        \n",
    "        ids = 1\n",
    "        \n",
    "\n",
    "        return ids, scores, bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat (1, 1024, 38, 50)\n",
      "anchor (28500, 4)\n",
      "rpn (1, 1024, 38, 50)\n",
      "rpn_score (1, 28500, 1)\n",
      "rpn_box_pred (1, 28500, 4)\n",
      "bbox_pred (28500, 4)\n",
      "anchor (28500, 4)\n",
      "[600, 800]\n",
      "width [  30.68011671   24.04203987   19.37640738 ...,  309.05692963  251.44822371\n",
      "  196.38676345]\n",
      "height [  19.21793151   23.62950611   30.953435   ...,  165.06685226  263.83555083\n",
      "  372.28306089]\n",
      "invalid [ True  True  True ..., False False False]\n",
      "scores (1, 28500, 1)\n",
      "scores (17529,)\n",
      "roi (28500, 4)\n",
      "roi (17529, 4)\n",
      "scores (17529,)\n",
      "keepindex (157,)\n",
      "keepindex (50,)\n",
      "pooled_feat (50, 1024, 14, 14)\n",
      "box_pred (80, 50, 4)\n",
      "rpn_box (50, 4)\n",
      "bbox (80, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "net = FasterRcnn()\n",
    "x = nd.random.uniform(shape=(1,1,600,800))\n",
    "net.initialize(init.Xavier())\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRcnn(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(None -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(None -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (4): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (5): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (top_features): HybridSequential(\n",
      "    (0): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=None)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  (class_predictor): Dense(None -> 81, linear)\n",
      "  (box_predictor): Dense(None -> 320, linear)\n",
      "  (rpn): HybridSequential(\n",
      "    (0): Conv2D(None -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Activation(relu)\n",
      "  )\n",
      "  (rpn_score): Conv2D(None -> 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_loc): Conv2D(None -> 60, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = FasterRcnn()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.features.load_parameters(\"features.params\")\n",
    "net.top_features.load_parameters(\"topfeatures.params\")\n",
    "#net.global_avg_pool.load_parameters(\"global_avg_pool.params\")\n",
    "net.class_predictor.load_parameters(\"class_predictor.params\")\n",
    "net.box_predictor.load_parameters(\"box_predictor.params\")\n",
    "net.rpn.load_parameters(\"rpn_conv1.params\")\n",
    "net.rpn_score.load_parameters(\"rpn_score.params\")\n",
    "net.rpn_loc.load_parameters(\"rpn_loc.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat (1, 1024, 38, 50)\n",
      "anchor (28500, 4)\n",
      "rpn (1, 1024, 38, 50)\n",
      "rpn_score (1, 28500, 1)\n",
      "rpn_box_pred (1, 28500, 4)\n",
      "bbox_pred (28500, 4)\n",
      "anchor (28500, 4)\n",
      "[600, 800]\n",
      "width [  31.99918604   37.17669296   25.42470777 ...,  357.45069581  221.3213045\n",
      "  179.91775751]\n",
      "height [  17.52285933   20.98014021   29.72794467 ...,  243.87935047  269.08685261\n",
      "  366.45209389]\n",
      "invalid [ True  True  True ..., False False False]\n",
      "scores (1, 28500, 1)\n",
      "scores (18211,)\n",
      "roi (28500, 4)\n",
      "roi (18211, 4)\n",
      "scores (18211,)\n",
      "keepindex (101,)\n",
      "keepindex (50,)\n",
      "pooled_feat (50, 1024, 14, 14)\n",
      "box_pred (80, 50, 4)\n",
      "rpn_box (50, 4)\n",
      "bbox (80, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "x = nd.random.uniform(shape=(1,3,600,800))\n",
    "y = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRcnn(\n",
      "  (features): HybridSequential(\n",
      "    (0): Conv2D(3 -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "    (2): Activation(relu)\n",
      "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)\n",
      "    (4): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(64 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=64)\n",
      "        (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "      )\n",
      "    )\n",
      "    (5): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=128)\n",
      "        (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "      )\n",
      "    )\n",
      "    (6): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (3): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (4): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "      (5): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=256)\n",
      "        (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=1024)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (top_features): HybridSequential(\n",
      "    (0): HybridSequential(\n",
      "      (0): BottleneckV1b(\n",
      "        (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        (downsample): HybridSequential(\n",
      "          (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "        )\n",
      "      )\n",
      "      (1): BottleneckV1b(\n",
      "        (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "      (2): BottleneckV1b(\n",
      "        (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (relu): Activation(relu)\n",
      "        (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=512)\n",
      "        (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, in_channels=2048)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (global_avg_pool): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True)\n",
      "  (class_predictor): Dense(2048 -> 81, linear)\n",
      "  (box_predictor): Dense(2048 -> 320, linear)\n",
      "  (rpn): HybridSequential(\n",
      "    (0): Conv2D(1024 -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Activation(relu)\n",
      "  )\n",
      "  (rpn_score): Conv2D(1024 -> 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (rpn_loc): Conv2D(1024 -> 60, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_img(imgname, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "    \"\"\"图像 nominalize \n",
    "    测试的时候，transform 不需要额外的处理，只需要归一化以及将 BCWH -> \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    imgs : string \n",
    "        待处理的图像名字\n",
    "    mean : iterable of float\n",
    "        Mean pixel values.\n",
    "    std : iterable of float\n",
    "        Standard deviations of pixel values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    网络的输入和 resize 后的图像\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    img = mx.image.imread(imgname)\n",
    "\n",
    "    # 只能在 600*800 时 work\n",
    "    img = mx.image.imresize(img, 600, 800)\n",
    "    orig_img = img.asnumpy().astype('uint8')\n",
    "    img = mx.nd.image.to_tensor(img)\n",
    "    img = mx.nd.image.normalize(img, mean=mean, std=std)\n",
    "    # img.expand_dims(0) 由 CHW -> BCHW\n",
    "    return img.expand_dims(0), orig_img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
